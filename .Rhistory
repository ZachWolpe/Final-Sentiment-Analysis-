axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
print(plt)
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print(head(results$sentiment_data))
print('---------------------------------------------------')
print('equity_dataset')
print(head(results$equity_dataset))
print('---------------------------------------------------')
print('plt')
print(head(results$plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
runApp('Sentiment')
# ______________________________________________________ Compute Sentiment Time Series ______________________________________________________
sentiment_time_series <- function(tweets, equity_dataset, cleaning_function=clean_tweets_text, start=Sys.Date()-365, end=Sys.Date(),
colour_1=randomColor(1), colour_2=randomColor(1)) {
"compute the sentiment overtime & contrast this with equity prices
Return:
- plt: visualization of equities & sentiment overtime (standardized)
- sentiment_data: dataset contain sentiment averaged over weeks
- equity_dataset: original equity dataset returned
Parameters:
- clean_twitter_dataset: unclean dataset
- cleaning_function: function to clean dataset
- equity_dataset:
- start & end: daterange
"
# _____________________ Sentiment per Week _____________________
# create weekly grouping
weekly <- tweets %>% filter(created_at >= start, created_at<=end) %>%
mutate(date = as.Date(created_at),
week = (as.numeric(date) %/% 7) - (as.numeric(min(date)) %/% 7)) %>%
select(text, week, created_at, screen_name)
# create a list (1 item per week) and clean text
clean_words_per_week <- mclapply(split(weekly, as.factor(weekly$week)), cleaning_function)
# compute bing sentiment for each item in the list
bing_sent_per_week <- mclapply(clean_words_per_week, compute_bing_sentiment)
net_bing_sent <- function(tibble) {mean(tibble$word_list$score)}                                # net sentiment function
ave_sent <- mclapply(bing_sent_per_week, net_bing_sent)                                         # apply net sent function
ave_sent <- tibble(week=(1:length(ave_sent)), sent=unlist(ave_sent))                            # compile results
print('------ ave_sent 1 ------')
print(head(ave_sent))
t <- weekly %>% group_by(week) %>% summarise(date = max(created_at))                            # correspond 'weeks' with dates (latest date per week)
t$week <- t$week + 1                                                                           # align tibbles
print('----- t ------')
print(head(t))
sent_per_date <- inner_join(ave_sent, t, by=c('week', 'week'))                                  # combine to create dates
sent_per_date$date <- as.Date(sent_per_date$date)                                               # convert date
# sent_per_date$sent <- na_kalman(sent_per_date$sent)                                           # Impute Missing Data
print('------ ave_sent ------')
print(head(ave_sent))
# _____________________ Visualize  _____________________
# standardize datasets
stnd <- function(data) (data-mean(data))/sd(data)
plt <- ggplot(sent_per_date, aes(x=date, y=stnd(sent))) +
ggtitle('Equities & Weekly Sentiment') +
geom_line(col=colour_1,linetype='dashed') + geom_point(aes(col=colour_1)) +
geom_line(data=equity_dataset, aes(x=date, y=stnd(adjusted)), linetype='dashed', col=colour_2) +
geom_point(data=equity_dataset, aes(x=date, y=stnd(adjusted), col=colour_2)) +
theme(axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
print(plt)
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print(head(results$sentiment_data))
print('---------------------------------------------------')
print('equity_dataset')
print(head(results$equity_dataset))
print('---------------------------------------------------')
print('plt')
print(head(results$plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
# ______________________________________________________ Compute Sentiment Time Series ______________________________________________________
sentiment_time_series <- function(tweets, equity_dataset, cleaning_function=clean_tweets_text, start=Sys.Date()-365, end=Sys.Date(),
colour_1=randomColor(1), colour_2=randomColor(1)) {
"compute the sentiment overtime & contrast this with equity prices
Return:
- plt: visualization of equities & sentiment overtime (standardized)
- sentiment_data: dataset contain sentiment averaged over weeks
- equity_dataset: original equity dataset returned
Parameters:
- clean_twitter_dataset: unclean dataset
- cleaning_function: function to clean dataset
- equity_dataset:
- start & end: daterange
"
# _____________________ Sentiment per Week _____________________
# create weekly grouping
weekly <- tweets %>% filter(created_at >= start, created_at<=end) %>%
mutate(date = as.Date(created_at),
week = (as.numeric(date) %/% 7) - (as.numeric(min(date)) %/% 7)) %>%
select(text, week, created_at, screen_name)
# create a list (1 item per week) and clean text
clean_words_per_week <- mclapply(split(weekly, as.factor(weekly$week)), cleaning_function)
# compute bing sentiment for each item in the list
bing_sent_per_week <- mclapply(clean_words_per_week, compute_bing_sentiment)
net_bing_sent <- function(tibble) {mean(tibble$word_list$score)}                                # net sentiment function
ave_sent <- mclapply(bing_sent_per_week, net_bing_sent)                                         # apply net sent function
ave_sent <- tibble(week=(1:length(ave_sent)), sent=unlist(ave_sent))                            # compile results
print('------ ave_sent 1 ------')
print(head(ave_sent))
t <- weekly %>% group_by(week) %>% summarise(date = max(created_at))                            # correspond 'weeks' with dates (latest date per week)
t$week <- t$week + 1                                                                           # align tibbles
print('----- t ------')
print(head(t))
sent_per_date <- inner_join(ave_sent, t, by=c('week', 'week'))                                  # combine to create dates
sent_per_date$date <- as.Date(sent_per_date$date)                                               # convert date
# sent_per_date$sent <- na_kalman(sent_per_date$sent)                                           # Impute Missing Data
print('------ ave_sent ------')
print(head(ave_sent))
# _____________________ Visualize  _____________________
# standardize datasets
stnd <- function(data) (data-mean(data))/sd(data)
plt <- ggplot(sent_per_date, aes(x=date, y=stnd(sent))) +
ggtitle('Equities & Weekly Sentiment') +
geom_line(col=colour_1,linetype='dashed') + geom_point(aes(col=colour_1)) +
geom_line(data=equity_dataset, aes(x=date, y=stnd(adjusted)), linetype='dashed', col=colour_2) +
geom_point(data=equity_dataset, aes(x=date, y=stnd(adjusted), col=colour_2)) +
theme(axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
print(plt)
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print(head(results$sentiment_data))
print('---------------------------------------------------')
print('equity_dataset')
print(head(results$equity_dataset))
print('---------------------------------------------------')
print('plt')
print(head(results$plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
runApp('Sentiment')
# ______________________________________________________ Compute Sentiment Time Series ______________________________________________________
sentiment_time_series <- function(tweets, equity_dataset, cleaning_function=clean_tweets_text, start=Sys.Date()-365, end=Sys.Date(),
colour_1=randomColor(1), colour_2=randomColor(1)) {
"compute the sentiment overtime & contrast this with equity prices
Return:
- plt: visualization of equities & sentiment overtime (standardized)
- sentiment_data: dataset contain sentiment averaged over weeks
- equity_dataset: original equity dataset returned
Parameters:
- clean_twitter_dataset: unclean dataset
- cleaning_function: function to clean dataset
- equity_dataset:
- start & end: daterange
"
# _____________________ Sentiment per Week _____________________
# create weekly grouping
weekly <- tweets %>% filter(created_at >= start, created_at<=end) %>%
mutate(date = as.Date(created_at),
week = (as.numeric(date) %/% 7) - (as.numeric(min(date)) %/% 7)) %>%
select(text, week, created_at, screen_name)
# create a list (1 item per week) and clean text
clean_words_per_week <- mclapply(split(weekly, as.factor(weekly$week)), cleaning_function)
# compute bing sentiment for each item in the list
bing_sent_per_week <- mclapply(clean_words_per_week, compute_bing_sentiment)
net_bing_sent <- function(tibble) {mean(tibble$word_list$score)}                                # net sentiment function
ave_sent <- mclapply(bing_sent_per_week, net_bing_sent)                                         # apply net sent function
ave_sent <- tibble(week=(1:length(ave_sent)), sent=unlist(ave_sent))                            # compile results
print('------ ave_sent 1 ------')
print(head(ave_sent))
t <- weekly %>% group_by(week) %>% summarise(date = max(created_at))                            # correspond 'weeks' with dates (latest date per week)
t$week <- t$week + 1                                                                           # align tibbles
print('----- t ------')
print(head(t))
print('-----merge-----')
print(head(merge(t, ave_sent, 'week')))
print(head(inner_join(ave_sent, t, by='week')))
print(head(inner_join(ave_sent, t, by=c('week','week'))))
sent_per_date <- inner_join(ave_sent, t, by=c('week', 'week'))                                  # combine to create dates
sent_per_date$date <- as.Date(sent_per_date$date)                                               # convert date
# sent_per_date$sent <- na_kalman(sent_per_date$sent)                                           # Impute Missing Data
print('------ ave_sent ------')
print(head(ave_sent))
# _____________________ Visualize  _____________________
# standardize datasets
stnd <- function(data) (data-mean(data))/sd(data)
plt <- ggplot(sent_per_date, aes(x=date, y=stnd(sent))) +
ggtitle('Equities & Weekly Sentiment') +
geom_line(col=colour_1,linetype='dashed') + geom_point(aes(col=colour_1)) +
geom_line(data=equity_dataset, aes(x=date, y=stnd(adjusted)), linetype='dashed', col=colour_2) +
geom_point(data=equity_dataset, aes(x=date, y=stnd(adjusted), col=colour_2)) +
theme(axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
print(plt)
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print(head(results$sentiment_data))
print('---------------------------------------------------')
print('equity_dataset')
print(head(results$equity_dataset))
print('---------------------------------------------------')
print('plt')
print(head(results$plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
runApp('Sentiment')
# ______________________________________________________ Compute Sentiment Time Series ______________________________________________________
sentiment_time_series <- function(tweets, equity_dataset, cleaning_function=clean_tweets_text, start=Sys.Date()-365, end=Sys.Date(),
colour_1=randomColor(1), colour_2=randomColor(1)) {
"compute the sentiment overtime & contrast this with equity prices
Return:
- plt: visualization of equities & sentiment overtime (standardized)
- sentiment_data: dataset contain sentiment averaged over weeks
- equity_dataset: original equity dataset returned
Parameters:
- clean_twitter_dataset: unclean dataset
- cleaning_function: function to clean dataset
- equity_dataset:
- start & end: daterange
"
# _____________________ Sentiment per Week _____________________
# create weekly grouping
weekly <- tweets %>% filter(created_at >= start, created_at<=end) %>%
mutate(date = as.Date(created_at),
week = (as.numeric(date) %/% 7) - (as.numeric(min(date)) %/% 7)) %>%
select(text, week, created_at, screen_name)
# create a list (1 item per week) and clean text
clean_words_per_week <- mclapply(split(weekly, as.factor(weekly$week)), cleaning_function)
# compute bing sentiment for each item in the list
bing_sent_per_week <- mclapply(clean_words_per_week, compute_bing_sentiment)
net_bing_sent <- function(tibble) {mean(tibble$word_list$score)}                                # net sentiment function
ave_sent <- mclapply(bing_sent_per_week, net_bing_sent)                                         # apply net sent function
ave_sent <- tibble(week=(1:length(ave_sent)), sent=unlist(ave_sent))                            # compile results
t <- weekly %>% group_by(week) %>% summarise(date = max(created_at))                            # correspond 'weeks' with dates (latest date per week)
t$week <- t$week + 1                                                                           # align tibbles
sent_per_date <- inner_join(ave_sent, t, by=c('week', 'week'))                                  # combine to create dates
sent_per_date$date <- as.Date(sent_per_date$date)                                               # convert date
# sent_per_date$sent <- na_kalman(sent_per_date$sent)                                           # Impute Missing Data
print('------ ave_sent ------')
print(head(sent_per_date))
# _____________________ Visualize  _____________________
# standardize datasets
stnd <- function(data) (data-mean(data))/sd(data)
plt <- ggplot(sent_per_date, aes(x=date, y=stnd(sent))) +
ggtitle('Equities & Weekly Sentiment') +
geom_line(col=colour_1,linetype='dashed') + geom_point(aes(col=colour_1)) +
geom_line(data=equity_dataset, aes(x=date, y=stnd(adjusted)), linetype='dashed', col=colour_2) +
geom_point(data=equity_dataset, aes(x=date, y=stnd(adjusted), col=colour_2)) +
theme(axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
print(plt)
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print(head(results$sentiment_data))
print('---------------------------------------------------')
print('equity_dataset')
print(head(results$equity_dataset))
print('---------------------------------------------------')
print('plt')
print(head(results$plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
# ______________________________________________________ Compute Sentiment Time Series ______________________________________________________
sentiment_time_series <- function(tweets, equity_dataset, cleaning_function=clean_tweets_text, start=Sys.Date()-365, end=Sys.Date(),
colour_1=randomColor(1), colour_2=randomColor(1)) {
"compute the sentiment overtime & contrast this with equity prices
Return:
- plt: visualization of equities & sentiment overtime (standardized)
- sentiment_data: dataset contain sentiment averaged over weeks
- equity_dataset: original equity dataset returned
Parameters:
- clean_twitter_dataset: unclean dataset
- cleaning_function: function to clean dataset
- equity_dataset:
- start & end: daterange
"
# _____________________ Sentiment per Week _____________________
# create weekly grouping
weekly <- tweets %>% filter(created_at >= start, created_at<=end) %>%
mutate(date = as.Date(created_at),
week = (as.numeric(date) %/% 7) - (as.numeric(min(date)) %/% 7)) %>%
select(text, week, created_at, screen_name)
# create a list (1 item per week) and clean text
clean_words_per_week <- mclapply(split(weekly, as.factor(weekly$week)), cleaning_function)
# compute bing sentiment for each item in the list
bing_sent_per_week <- mclapply(clean_words_per_week, compute_bing_sentiment)
net_bing_sent <- function(tibble) {mean(tibble$word_list$score)}                                # net sentiment function
ave_sent <- mclapply(bing_sent_per_week, net_bing_sent)                                         # apply net sent function
ave_sent <- tibble(week=(1:length(ave_sent)), sent=unlist(ave_sent))                            # compile results
t <- weekly %>% group_by(week) %>% summarise(date = max(created_at))                            # correspond 'weeks' with dates (latest date per week)
t$week <- t$week + 1                                                                           # align tibbles
sent_per_date <- inner_join(ave_sent, t, by=c('week', 'week'))                                  # combine to create dates
sent_per_date$date <- as.Date(sent_per_date$date)                                               # convert date
# sent_per_date$sent <- na_kalman(sent_per_date$sent)                                           # Impute Missing Data
print('------ ave_sent ------')
print(head(sent_per_date))
# _____________________ Visualize  _____________________
# standardize datasets
stnd <- function(data) (data-mean(data))/sd(data)
plt <- ggplot(sent_per_date, aes(x=date, y=stnd(sent))) +
ggtitle('Equities & Weekly Sentiment') +
geom_line(col=colour_1,linetype='dashed') + geom_point(aes(col=colour_1)) +
geom_line(data=equity_dataset, aes(x=date, y=stnd(adjusted)), linetype='dashed', col=colour_2) +
geom_point(data=equity_dataset, aes(x=date, y=stnd(adjusted), col=colour_2)) +
theme(axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
print(plt)
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print(head(results$sentiment_data))
print('---------------------------------------------------')
print('equity_dataset')
print(head(results$equity_dataset))
print('---------------------------------------------------')
print('plt')
print(head(results$plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
runApp('Sentiment')
# ______________________________________________________ Compute Sentiment Time Series ______________________________________________________
sentiment_time_series <- function(tweets, equity_dataset, cleaning_function=clean_tweets_text, start=Sys.Date()-365, end=Sys.Date(),
colour_1=randomColor(1), colour_2=randomColor(1)) {
"compute the sentiment overtime & contrast this with equity prices
Return:
- plt: visualization of equities & sentiment overtime (standardized)
- sentiment_data: dataset contain sentiment averaged over weeks
- equity_dataset: original equity dataset returned
Parameters:
- clean_twitter_dataset: unclean dataset
- cleaning_function: function to clean dataset
- equity_dataset:
- start & end: daterange
"
# _____________________ Sentiment per Week _____________________
# create weekly grouping
weekly <- tweets %>% filter(created_at >= start, created_at<=end) %>%
mutate(date = as.Date(created_at),
week = (as.numeric(date) %/% 7) - (as.numeric(min(date)) %/% 7)) %>%
select(text, week, created_at, screen_name)
# create a list (1 item per week) and clean text
clean_words_per_week <- mclapply(split(weekly, as.factor(weekly$week)), cleaning_function)
# compute bing sentiment for each item in the list
bing_sent_per_week <- mclapply(clean_words_per_week, compute_bing_sentiment)
net_bing_sent <- function(tibble) {mean(tibble$word_list$score)}                                # net sentiment function
ave_sent <- mclapply(bing_sent_per_week, net_bing_sent)                                         # apply net sent function
ave_sent <- tibble(week=(1:length(ave_sent)), sent=unlist(ave_sent))                            # compile results
t <- weekly %>% group_by(week) %>% summarise(date = max(created_at))                            # correspond 'weeks' with dates (latest date per week)
t$week <- t$week + 1                                                                           # align tibbles
sent_per_date <- inner_join(ave_sent, t, by=c('week', 'week'))                                  # combine to create dates
sent_per_date$date <- as.Date(sent_per_date$date)                                               # convert date
# sent_per_date$sent <- na_kalman(sent_per_date$sent)                                           # Impute Missing Data
print('------ ave_sent ------')
print(head(sent_per_date))
# _____________________ Visualize  _____________________
# standardize datasets
stnd <- function(data) (data-mean(data))/sd(data)
plt <- ggplot(sent_per_date, aes(x=date, y=stnd(sent))) +
ggtitle('Equities & Weekly Sentiment') +
geom_line(col=colour_1,linetype='dashed') + geom_point(aes(col=colour_1)) +
geom_line(data=equity_dataset, aes(x=date, y=stnd(adjusted)), linetype='dashed', col=colour_2) +
geom_point(data=equity_dataset, aes(x=date, y=stnd(adjusted), col=colour_2)) +
theme(axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
print(plt)
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print((results$sentiment_data))
print('---------------------------------------------------')
print('equity_dataset')
print((results$equity_dataset))
print('---------------------------------------------------')
print('plt')
print((results$plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
runApp('Sentiment')
# ______________________________________________________ Compute Sentiment Time Series ______________________________________________________
sentiment_time_series <- function(tweets, equity_dataset, cleaning_function=clean_tweets_text, start=Sys.Date()-365, end=Sys.Date(),
colour_1=randomColor(1), colour_2=randomColor(1)) {
"compute the sentiment overtime & contrast this with equity prices
Return:
- plt: visualization of equities & sentiment overtime (standardized)
- sentiment_data: dataset contain sentiment averaged over weeks
- equity_dataset: original equity dataset returned
Parameters:
- clean_twitter_dataset: unclean dataset
- cleaning_function: function to clean dataset
- equity_dataset:
- start & end: daterange
"
# _____________________ Sentiment per Week _____________________
# create weekly grouping
weekly <- tweets %>% filter(created_at >= start, created_at<=end) %>%
mutate(date = as.Date(created_at),
week = (as.numeric(date) %/% 7) - (as.numeric(min(date)) %/% 7)) %>%
select(text, week, created_at, screen_name)
# create a list (1 item per week) and clean text
clean_words_per_week <- mclapply(split(weekly, as.factor(weekly$week)), cleaning_function)
# compute bing sentiment for each item in the list
bing_sent_per_week <- mclapply(clean_words_per_week, compute_bing_sentiment)
net_bing_sent <- function(tibble) {mean(tibble$word_list$score)}                                # net sentiment function
ave_sent <- mclapply(bing_sent_per_week, net_bing_sent)                                         # apply net sent function
ave_sent <- tibble(week=(1:length(ave_sent)), sent=unlist(ave_sent))                            # compile results
t <- weekly %>% group_by(week) %>% summarise(date = max(created_at))                            # correspond 'weeks' with dates (latest date per week)
t$week <- t$week + 1                                                                           # align tibbles
sent_per_date <- inner_join(ave_sent, t, by=c('week', 'week'))                                  # combine to create dates
sent_per_date$date <- as.Date(sent_per_date$date)                                               # convert date
sent_per_date$sent <- na_kalman(sent_per_date$sent)                                           # Impute Missing Data
# _____________________ Visualize  _____________________
# standardize datasets
stnd <- function(data) (data-mean(data))/sd(data)
plt <- ggplot(sent_per_date, aes(x=date, y=stnd(sent))) +
ggtitle('Equities & Weekly Sentiment') +
geom_line(col=colour_1,linetype='dashed') + geom_point(aes(col=colour_1)) +
geom_line(data=equity_dataset, aes(x=date, y=stnd(adjusted)), linetype='dashed', col=colour_2) +
geom_point(data=equity_dataset, aes(x=date, y=stnd(adjusted), col=colour_2)) +
theme(axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
legend.title = element_blank()) +
scale_color_manual(values=c(colour_2, colour_1), labels=c('equities', 'sentiment'))
# store results
results <- list()
results[['sentiment_data']] <- sent_per_date
results[['equity_dataset']] <- equity_dataset
results[['plt']] <- plt
print('---------------------------------------------------')
print('sentiment_data')
print(head(sent_per_date))
print('---------------------------------------------------')
print('equity_dataset')
print(head(equity_dataset))
print('---------------------------------------------------')
print('plt')
print((plt))
# _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  To Do  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
#  - compute metrics
return(results)
}
runApp('Sentiment')
